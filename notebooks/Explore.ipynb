{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc08a90",
   "metadata": {},
   "source": [
    "# Computer Vision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc37ea",
   "metadata": {},
   "source": [
    "#### Approach we will take for the Aerial vs Ground Natural Disaster computer vision project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bc598",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8880aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this from GitHub Repository / Random Class Notebooks / etc.\n",
    "\n",
    "# At minimum, we need PyTorch\n",
    "\n",
    "# Run this cell only if working in Colab\n",
    "# Connects to any needed files from GitHub and Google Drive\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Remove Colab default sample_data\n",
    "!rm -r ./sample_data\n",
    "\n",
    "# # Clone GitHub files to colab workspace\n",
    "git_user = \"sfhorng\" # Enter user or organization name\n",
    "git_email = \"sh390@duke.edu\" # Enter your email\n",
    "repo_name = \"AIPI-540-CV-Team-2-Project\" # Enter repo name\n",
    "# # Use the below if repo is private, or is public and you want to push to it\n",
    "# # Otherwise comment next two lines out\n",
    "git_token = getpass.getpass(\"enter git token\") # Enter your github token \n",
    "git_path = f\"https://{git_token}@github.com/{git_user}/{repo_name}.git\"\n",
    "!git clone \"{git_path}\"\n",
    "\n",
    "# Install dependencies from requirements.txt file\n",
    "notebook_dir = 'notebooks'\n",
    "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
    "\n",
    "# Change working directory to location of notebook\n",
    "\n",
    "path_to_data = os.path.join(repo_name,'data/raw')\n",
    "%cd \"{path_to_data}\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da624728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e33435",
   "metadata": {},
   "source": [
    "## Load data to our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the following notebooks:\n",
    "# Image classification (writing neural network from scratch)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "## AIDER dataset\n",
    "# Stephanie\n",
    "# path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/Filtered/AIDER_filtered.zip\"\n",
    "#Amani\n",
    "path=\"/content/drive/My Drive/AIDER_filtered.zip\"\n",
    "zip_ref = zipfile.ZipFile(path, 'r')\n",
    "zip_ref.extractall(os.getcwd())\n",
    "zip_ref.close()\n",
    "\n",
    "## MEDIC dataset\n",
    "# Stephanie\n",
    "# path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/MEDIC.tar.gz\"\n",
    "\n",
    "gz_file = tarfile.open(path)\n",
    "gz_file.extractall(os.getcwd())\n",
    "gz_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855c406",
   "metadata": {},
   "source": [
    "## Data Preparation: \n",
    "### Set up class to add labels, manipulate the images to size correctly, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecad6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIDER - use approach based on image classification notebook (writing neural network from scratch)\n",
    "# MEDIC - need to use DICOM\n",
    "\n",
    "# labels = Aerial and Ground. Need to standardize \"Fire\", \"Flood\", \"No Disaster\" across both datasets so labels are the same\n",
    "\n",
    "# Reference code to size images correctly. May not be in correct section, \n",
    "# but we might need to compare image sizes across our two data sets and standardize?\n",
    "\n",
    "# Data augmentation to create new images from AIDER data set to get better class balance between aerial and ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c26571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up transformations for training and validation (test) data\n",
    "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
    "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for training and validation sets--AIDER\n",
    "train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/val'), data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b267bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "class CustomDataset(Dataset):\n",
    "    '''\n",
    "    Custom PyTorch Dataset for image classification\n",
    "    Must contain 3 parts: __init__, __len__ and __getitem__\n",
    "    '''\n",
    "\n",
    "    def __init__(self, labels_df: DataFrame, data_dir: str, class_mapper: dict, transform=None):\n",
    "        '''\n",
    "        Args:\n",
    "            labels_df (DataFrame): Dataframe containing the image names and corresponding labels\n",
    "            data_dir (string): Path to directory containing the images\n",
    "            class_mapper (dict): Dictionary mapping string labels to numeric labels\n",
    "            transform (callable,optional): Optional transform to be applied to images\n",
    "        '''\n",
    "        self.labels_df = labels_df\n",
    "        self.transform = transform\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = self.labels_df['disaster_types'].unique()\n",
    "        self.classmapper = class_mapper\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Returns the number of images in the Dataset'''\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns the image and corresponding label for an input index\n",
    "        Used by PyTorch to create the iterable DataLoader\n",
    "\n",
    "        Args:\n",
    "            idx (integer): index value for which to get image and label\n",
    "        '''\n",
    "        # Load the image\n",
    "        img_path = os.path.join(self.data_dir,\n",
    "                                self.labels_df.iloc[idx, 2])\n",
    "        # For a normal image file (jpg,png) use the below\n",
    "        image = cv2.imread(img_path) # Use this for normal color images\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Use this for color images to rearrange channels BGR -> RGB\n",
    "        image = Image.fromarray(image) # convert numpy array to PIL image\n",
    "\n",
    "        # Load the label\n",
    "        label = self.labels_df.iloc[idx, -1]\n",
    "        label = self.classmapper[label]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715015e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filtered_medic_df(type_dataset):\n",
    "    medic_df = pd.read_csv(os.path.join(os.getcwd(),f'MEDIC/MEDIC_{type_dataset}.tsv'), sep='\\t', header=0)\n",
    "    filtered_medic_df = medic_df[medic_df['disaster_types'].isin(['fire', 'flood', 'not_disaster'])]\n",
    "    filtered_medic_df = filtered_medic_df[filtered_medic_df['damage_severity'] != 'mild']\n",
    "    filtered_medic_df = filtered_medic_df[filtered_medic_df['informative'] == 'informative']\n",
    "    display(filtered_medic_df[['image_id', 'disaster_types']])\n",
    "    return filtered_medic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497b0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train_medic_df = generate_filtered_medic_df('train')\n",
    "filtered_val_medic_df = generate_filtered_medic_df('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3190e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['fire','flood', 'not_disaster']\n",
    "idx_to_class = {i:j for i,j in enumerate(classes)}\n",
    "class_to_idx = {v:k for k,v in idx_to_class.items()}\n",
    "\n",
    "# need to restructure code so that train_dataset and val_dataset \n",
    "    # can be reused w/out overwriting\n",
    "\n",
    "train_dataset = CustomDataset(labels_df=filtered_train_medic_df,\n",
    "                            data_dir=os.path.join(os.getcwd(), 'MEDIC'),\n",
    "                            class_mapper=class_to_idx,\n",
    "                            transform = data_transforms['train'])\n",
    "\n",
    "val_dataset = CustomDataset(labels_df=filtered_val_medic_df,\n",
    "                            data_dir=os.path.join(os.getcwd(), 'MEDIC'),\n",
    "                            class_mapper=class_to_idx,\n",
    "                            transform = data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "#num_workers:tells dataloader instane how many sub-processes to use for data loading. If zero, GPU has to weight for CPU\n",
    "#load data.greater num_workers more efficiently the CPU load data and less the GPU has to wait. \n",
    "#Google Colab: suggested num_workers=2\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "# Set up dict for dataloaders\n",
    "dataloaders = {'train':train_loader,'val':val_loader}\n",
    "# Store size of training and validation sets\n",
    "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
    "# Get class names associated with labels\n",
    "class_names = train_dataset.classes\n",
    "#print(class_names)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b41a4d",
   "metadata": {},
   "source": [
    "Images-AIDER-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(train_loader).next()\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb74d3",
   "metadata": {},
   "source": [
    "Images-AIDER-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(val_loader).next()\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dbd47",
   "metadata": {},
   "source": [
    "# Amani:\n",
    "## Set up Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b14206",
   "metadata": {},
   "source": [
    "ResNet50:CNN's have a major disadvantage-'Vanishing Gradient Problem';recall that during backpropagation, the value of gradient decreases significantly, thus hardly any change occurs to the weights.\n",
    "##ResNet is used to make use of the \"Skip Connection\"\n",
    "##Skip connection-adding the orginal input to the output of the convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd45a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pre-trained resnet\n",
    "net = torchvision.models.resnet50(pretrained=True)\n",
    "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Display a summary of the layers of the model and output shape after each layer\n",
    "summary(net,(images.shape[1:]),batch_size=batch_size,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8506ee",
   "metadata": {},
   "source": [
    "##Instantiate the model and display(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bde8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of inputs to final Linear layer\n",
    "num_ftrs = net.fc.in_features\n",
    "#print(num_ftrs)\n",
    "# Replace final Linear layer with a new Linear with the same number of inputs but just 3 outputs,\n",
    "# since we have 3 classes - fire, flood and normal or diaster\n",
    "net.fc = nn.Linear(num_ftrs, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f714af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23567",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ad0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, scheduler, device, num_epochs=25):\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the input images and labels, and send to GPU if available\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the weight gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass to get outputs and calculate loss\n",
    "                # Track gradient only for training data\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagation to get the gradients with respect to each weight\n",
    "                    # Only if in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Update the weights\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Convert loss into a scalar and add it to running_loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # Track number of correct predictions\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Step along learning rate scheduler when in train\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Calculate and display average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # If model performs better on val set, save weights as the best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:3f}'.format(best_acc))\n",
    "\n",
    "    # Load the weights from best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler - decay LR by a factor of 0.1 every 7 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Train the model\n",
    "net = train_model(net, criterion, optimizer, dataloaders, lr_scheduler, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a batch of predictions\n",
    "def visualize_results(model,dataloader,device):\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Get a batch of validation images\n",
    "        images, labels = iter(val_loader).next()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Get predictions\n",
    "        _,preds = torch.max(model(images), 1)\n",
    "        preds = np.squeeze(preds.cpu().numpy())\n",
    "        images = images.cpu().numpy()\n",
    "\n",
    "    # Plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for idx in np.arange(len(preds)):\n",
    "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
    "        image = images[idx]\n",
    "        image = image.transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
    "    return\n",
    "\n",
    "visualize_results(net,val_loader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2316d",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57da028",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0dea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEDIC:\n",
    "## Choose which pictures to keep from MEDIC (there are a lot more than AIDER)\n",
    "## Drop pictures we don't want from MEDIC\n",
    "## Drop Mild Category\n",
    "## Drop Not Informative label if they exist?\n",
    "## Only Keep fires, floods, and not disasters\n",
    "\n",
    "#AIDER:\n",
    "## Only keep fires, floods and not disasters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45182f18",
   "metadata": {},
   "source": [
    "## Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a0b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge MEDIC and AIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0cefa",
   "metadata": {},
   "source": [
    "## Train_test_split into our training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42be63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7fb363",
   "metadata": {},
   "source": [
    "# Amani:\n",
    "## Set up Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f35855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to decide on best architecture to use based on our goals. Performance/computational resources tradeoffs\n",
    "\n",
    "# Are there pre-trained models that are less computationally heavy to start off with? Research this & include in PowerPoint\n",
    "# Transfer Learning ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc94e471",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3b234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference existing code to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bc32f",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a7e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation / Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe68cd",
   "metadata": {},
   "source": [
    "## Report on Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b497e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98f584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground vs Aerial analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6b7b02a527123a58752ed98c09a696410bd0a26e4f23108e96487035cbed0e2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
