{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc08a90",
   "metadata": {},
   "source": [
    "# Computer Vision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc37ea",
   "metadata": {},
   "source": [
    "#### Approach we will take for the Aerial vs Ground Natural Disaster computer vision project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bc598",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8880aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this from GitHub Repository / Random Class Notebooks / etc.\n",
    "\n",
    "# At minimum, we need PyTorch\n",
    "\n",
    "# Run this cell only if working in Colab\n",
    "# Connects to any needed files from GitHub and Google Drive\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Remove Colab default sample_data\n",
    "!rm -r ./sample_data\n",
    "\n",
    "# # Clone GitHub files to colab workspace\n",
    "git_user = \"sfhorng\" # Enter user or organization name\n",
    "git_email = \"sh390@duke.edu\" # Enter your email\n",
    "repo_name = \"AIPI-540-CV-Team-2-Project\" # Enter repo name\n",
    "# # Use the below if repo is private, or is public and you want to push to it\n",
    "# # Otherwise comment next two lines out\n",
    "git_token = getpass.getpass(\"enter git token\") # Enter your github token \n",
    "git_path = f\"https://{git_token}@github.com/{git_user}/{repo_name}.git\"\n",
    "!git clone \"{git_path}\"\n",
    "\n",
    "# Install dependencies from requirements.txt file\n",
    "notebook_dir = 'notebooks'\n",
    "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
    "\n",
    "# Change working directory to location of notebook\n",
    "\n",
    "path_to_data = os.path.join(repo_name,'data/raw')\n",
    "%cd \"{path_to_data}\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da624728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e33435",
   "metadata": {},
   "source": [
    "## Load data to our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the following notebooks:\n",
    "\n",
    "\n",
    "# Image classification (writing neural network from scratch)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Stephanie\n",
    "# path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/Filtered/AIDER_filtered.zip\"\n",
    "#Amani\n",
    "path=\"/content/drive/My Drive/AIDER_filtered.zip\"\n",
    "zip_ref = zipfile.ZipFile(path, 'r')\n",
    "zip_ref.extractall(os.path.join(os.getcwd()))\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855c406",
   "metadata": {},
   "source": [
    "## Data Preparation: \n",
    "### Set up class to add labels, manipulate the images to size correctly, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecad6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIDER - use approach based on image classification notebook (writing neural network from scratch)\n",
    "# MEDIC - need to use DICOM\n",
    "\n",
    "# labels = Aerial and Ground. Need to standardize \"Fire\", \"Flood\", \"No Disaster\" across both datasets so labels are the same\n",
    "\n",
    "# Reference code to size images correctly. May not be in correct section, \n",
    "# but we might need to compare image sizes across our two data sets and standardize?\n",
    "\n",
    "# Data augmentation to create new images from AIDER data set to get better class balance between aerial and ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c26571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up transformations for training and validation (test) data\n",
    "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
    "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
    "#val_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for traingin and validation sets--AIDER\n",
    "train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "#num_workers:tells dataloader instane how many sub-processes to use for data loading. If zero, GPU has to weight for CPU\n",
    "#load data.greater num_workers more efficiently the CPU load data and less the GPU has to wait. \n",
    "#Google Colab: suggested num_workers=2\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "# Set up dict for dataloaders\n",
    "dataloaders = {'train':train_loader,'val':val_loader}\n",
    "# Store size of training and validation sets\n",
    "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
    "# Get class names associated with labels\n",
    "class_names = train_dataset.classes\n",
    "#print(class_names)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b41a4d",
   "metadata": {},
   "source": [
    "Images-AIDER-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(train_loader).next()\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbb74d3",
   "metadata": {},
   "source": [
    "Images-AIDER-Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377bae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(val_loader).next()\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dbd47",
   "metadata": {},
   "source": [
    "# Amani:\n",
    "## Set up Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b14206",
   "metadata": {},
   "source": [
    "ResNet50:CNN's have a major disadvantage-'Vanishing Gradient Problem';recall that during backpropagation, the value of gradient decreases significantly, thus hardly any change occurs to the weights.\n",
    "##ResNet is used to make use of the \"Skip Connection\"\n",
    "##Skip connection-adding the orginal input to the output of the convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a841a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetResize(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        ### DOWNSCALING LAYER ###\n",
    "\n",
    "        # Conv layer: (3,224,224) -> (8,224,224)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # Conv layer output size = (W-F+2P)/S+1 = (224-3+2)/1+1 = 224\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        ### RESNET BLOCK 1 ###\n",
    "\n",
    "        # Main path\n",
    "        # Conv layer: (8,224,224) -> (16,112,112)\n",
    "        self.block1conv1 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        self.block1bn1 = nn.BatchNorm2d(16)\n",
    "        # Conv layer: (16,112,112) -> (16,112,112)\n",
    "        self.block1conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.block1bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Skip connection: (8,224,224) -> (16,112,112)\n",
    "        self.block1skipconv = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "        self.block1skipbn = nn.BatchNorm2d(16)\n",
    "\n",
    "        ### RESNET BLOCK 2 ###\n",
    "\n",
    "        # Main path\n",
    "        # Conv layer: (16,112,112) -> (32,56,56)\n",
    "        self.block2conv1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.block2bn1 = nn.BatchNorm2d(32)\n",
    "        # Conv layer: (32,56,56) -> (32,56,56)\n",
    "        self.block2conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.block2bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Skip connection: (16,112,112) -> (32,56,56)\n",
    "        self.block2skipconv = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)\n",
    "        self.block2skipbn = nn.BatchNorm2d(32)\n",
    "\n",
    "        ### FINAL LAYERS ###\n",
    "\n",
    "        # Average pooling layer: (32,56,56) -> (32,8,8)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=7, stride=7)\n",
    "        \n",
    "        # Input size: 32 * 8 * 8 =  from pooling layer$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "        # 2 output channels (for the 2 classes)put 3\n",
    "        self.fc1 = nn.Linear(32*8*8, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        ### DOWNSCALING LAYER ###\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        ### RESNET BLOCK 1 ###\n",
    "        skipconnect = x\n",
    "        skipconnect = self.block1skipconv(skipconnect)\n",
    "        skipconnect = self.block1skipbn(skipconnect)\n",
    "\n",
    "        x_out = self.block1conv1(x) # conv1\n",
    "        x_out = self.block1bn1(x_out) # batch norm 1\n",
    "        x_out = F.relu(x_out) # relu\n",
    "\n",
    "        x_out = self.block1conv2(x_out) # conv2\n",
    "        x_out = self.block1bn2(x_out) # batch norm 2\n",
    "\n",
    "        # Add layer and skipconnect, then activation\n",
    "        x_out += skipconnect\n",
    "        x_out = F.relu(x_out)\n",
    "\n",
    "        ### RESNET BLOCK 2 ###\n",
    "        skipconnect = x_out\n",
    "        skipconnect = self.block2skipconv(skipconnect)\n",
    "        skipconnect = self.block2skipbn(skipconnect)\n",
    "\n",
    "        x_out = self.block2conv1(x_out) # conv1\n",
    "        x_out = self.block2bn1(x_out) # batch norm 1\n",
    "        x_out = F.relu(x_out) # relu\n",
    "\n",
    "        x_out = self.block2conv2(x_out) # conv2\n",
    "        x_out = self.block2bn2(x_out) # batch norm 2\n",
    "\n",
    "        # Add layer and skipconnect, then activation\n",
    "        x_out += skipconnect\n",
    "        x_out = F.relu(x_out)\n",
    "\n",
    "        ### FINAL LAYERS ###\n",
    "        x_out = self.pool2(x_out)\n",
    "        # Flatten into a vector to feed into linear layer\n",
    "        x_out = x_out.view(x_out.size(0), -1)\n",
    "        # Linear layer\n",
    "        x_out = self.fc1(x_out)\n",
    "        \n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8506ee",
   "metadata": {},
   "source": [
    "##Instantiate the model and display(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa27f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "net = ResNetResize()\n",
    "\n",
    "# Display a summary of the layers of the model and output shape after each layer\n",
    "summary(net,(images.shape[1:]),batch_size=batch_size,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23567",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=60):\n",
    "\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "\n",
    "    iter_num = {'train':0,'val':0} # Track total number of iterations\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the input images and labels, and send to GPU if available\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the weight gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass to get outputs and calculate loss\n",
    "                # Track gradient only for training data\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagation to get the gradients with respect to each weight\n",
    "                    # Only if in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Update the weights\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Convert loss into a scalar and add it to running_loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # Track number of correct predictions\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Iterate count of iterations\n",
    "                iter_num[phase] += 1\n",
    "\n",
    "            # Calculate and display average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the model\n",
    "net = ResNetResize()\n",
    "\n",
    "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(),  lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_model(net, criterion, optimizer, dataloaders, device, num_epochs=60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f2316d",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f590a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a batch of predictions\n",
    "\n",
    "def visualize_results(model,dataloader,device):\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # Get a batch of validation images\n",
    "        images, labels = iter(val_loader).next()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # Get predictions\n",
    "        _,preds = torch.max(model(images), 1)\n",
    "        preds = np.squeeze(preds.cpu().numpy())\n",
    "        images = images.cpu().numpy()\n",
    "\n",
    "    # Plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    for idx in np.arange(len(preds)):\n",
    "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
    "        image = images[idx]\n",
    "        image = image.transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(\"{} ({})\".format(class_names[preds[idx]], class_names[labels[idx]]),\n",
    "                    color=(\"green\" if preds[idx]==labels[idx] else \"red\"))\n",
    "    return\n",
    "\n",
    "visualize_results(net,val_loader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57da028",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0dea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEDIC:\n",
    "## Choose which pictures to keep from MEDIC (there are a lot more than AIDER)\n",
    "## Drop pictures we don't want from MEDIC\n",
    "## Drop Mild Category\n",
    "## Drop Not Informative label if they exist?\n",
    "## Only Keep fires, floods, and not disasters\n",
    "\n",
    "#AIDER:\n",
    "## Only keep fires, floods and not disasters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45182f18",
   "metadata": {},
   "source": [
    "## Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a0b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge MEDIC and AIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0cefa",
   "metadata": {},
   "source": [
    "## Train_test_split into our training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42be63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dbd47",
   "metadata": {},
   "source": [
    "# Amani:\n",
    "## Set up Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f35855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to decide on best architecture to use based on our goals. Performance/computational resources tradeoffs\n",
    "\n",
    "# Are there pre-trained models that are less computationally heavy to start off with? Research this & include in PowerPoint\n",
    "# Transfer Learning ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23567",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3b234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference existing code to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bc32f",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a7e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation / Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe68cd",
   "metadata": {},
   "source": [
    "## Report on Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b497e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98f584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground vs Aerial analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6b7b02a527123a58752ed98c09a696410bd0a26e4f23108e96487035cbed0e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearningSummer2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
