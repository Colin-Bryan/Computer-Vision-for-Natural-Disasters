{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7dc08a90",
      "metadata": {
        "id": "7dc08a90"
      },
      "source": [
        "# Computer Vision Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6cc37ea",
      "metadata": {
        "id": "a6cc37ea"
      },
      "source": [
        "#### Approach we will take for the Aerial vs Ground Natural Disaster computer vision project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486bc598",
      "metadata": {
        "id": "486bc598"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8880aa1",
      "metadata": {
        "id": "b8880aa1"
      },
      "outputs": [],
      "source": [
        "# Get this from GitHub Repository / Random Class Notebooks / etc.\n",
        "\n",
        "# At minimum, we need PyTorch\n",
        "\n",
        "# Run this cell only if working in Colab\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "!rm -r ./sample_data\n",
        "\n",
        "# # Clone GitHub files to colab workspace\n",
        "git_user = \"sfhorng\" # Enter user or organization name\n",
        "git_email = \"sh390@duke.edu\" # Enter your email\n",
        "repo_name = \"AIPI-540-CV-Team-2-Project\" # Enter repo name\n",
        "# # Use the below if repo is private, or is public and you want to push to it\n",
        "# # Otherwise comment next two lines out\n",
        "git_token = getpass.getpass(\"enter git token\") # Enter your github token \n",
        "git_path = f\"https://{git_token}@github.com/{git_user}/{repo_name}.git\"\n",
        "!git clone \"{git_path}\"\n",
        "\n",
        "# Install dependencies from requirements.txt file\n",
        "notebook_dir = 'notebooks'\n",
        "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
        "\n",
        "# Change working directory to location of notebook\n",
        "\n",
        "path_to_data = os.path.join(repo_name,'data/raw')\n",
        "%cd \"{path_to_data}\"\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da624728",
      "metadata": {
        "id": "da624728"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import copy\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchsummary import summary\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2 as cv \n",
        "from PIL import Image\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e33435",
      "metadata": {
        "id": "e5e33435"
      },
      "source": [
        "## Load data  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356d08df",
      "metadata": {
        "id": "356d08df"
      },
      "outputs": [],
      "source": [
        "# Reference the following notebooks:\n",
        "# Image classification (writing neural network from scratch)\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "## AIDER dataset\n",
        "# Stephanie\n",
        "#path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/Filtered/AIDER_filtered.zip\"\n",
        "\n",
        "#Amani\n",
        "path=\"/content/drive/My Drive/AIDER_filtered.zip\"\n",
        "zip_ref = zipfile.ZipFile(path, 'r')\n",
        "zip_ref.extractall(os.getcwd())\n",
        "zip_ref.close()\n",
        "path = \"/content/drive/My Drive/data_disaster_types.tar.gz\"\n",
        "gz_file = tarfile.open(path)\n",
        "gz_file.extractall(os.getcwd())\n",
        "\n",
        "## MEDIC dataset\n",
        "# Stephanie\n",
        "#path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/data_disaster_types.zip\"\n",
        "\n",
        "#zip_ref = zipfile.ZipFile(path, 'r')\n",
        "#zip_ref.extractall(os.getcwd())\n",
        "#zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9855c406",
      "metadata": {
        "id": "9855c406"
      },
      "source": [
        "## Data Preparation\n",
        "### Set up class to add labels, manipulate the images to size correctly, data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c26571",
      "metadata": {
        "id": "f6c26571"
      },
      "outputs": [],
      "source": [
        "# Set up transformations for training and validation (test) data\n",
        "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
        "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
        "def get_data_transforms():\n",
        "    data_transforms = {\n",
        "      'train': transforms.Compose([\n",
        "          transforms.RandomResizedCrop(224),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "      'val': transforms.Compose([\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(224),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "      ]),\n",
        "    }\n",
        "    return data_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4OK9UC0vmd73",
      "metadata": {
        "id": "4OK9UC0vmd73"
      },
      "source": [
        "Data Set: MEDIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f6e038",
      "metadata": {
        "id": "38f6e038"
      },
      "outputs": [],
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "class CustomDataset(Dataset):\n",
        "    '''\n",
        "    Custom PyTorch Dataset for image classification\n",
        "    Must contain 3 parts: __init__, __len__ and __getitem__\n",
        "    '''\n",
        "\n",
        "    def __init__(self, labels_df: DataFrame, data_dir: str, class_mapper: dict, transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "            labels_df (DataFrame): Dataframe containing the image names and corresponding labels\n",
        "            data_dir (string): Path to directory containing the images\n",
        "            class_mapper (dict): Dictionary mapping string labels to numeric labels\n",
        "            transform (callable,optional): Optional transform to be applied to images\n",
        "        '''\n",
        "        self.labels_df = labels_df\n",
        "        self.transform = transform\n",
        "        self.data_dir = data_dir\n",
        "        self.classes = self.labels_df['class_label'].unique()\n",
        "        self.classmapper = class_mapper\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Returns the number of images in the Dataset'''\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Returns the image and corresponding label for an input index\n",
        "        Used by PyTorch to create the iterable DataLoader\n",
        "\n",
        "        Args:\n",
        "            idx (integer): index value for which to get image and label\n",
        "        '''\n",
        "        # Load the image\n",
        "        img_path = os.path.join(self.data_dir,\n",
        "                                self.labels_df.iloc[idx, 0])\n",
        "\n",
        "        # For a normal image file (jpg,png) use the below\n",
        "        image = cv2.imread(img_path) # Use this for normal color images\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Use this for color images to rearrange channels BGR -> RGB\n",
        "        image = Image.fromarray(image) # convert numpy array to PIL image\n",
        "\n",
        "        # Load the label\n",
        "        label = self.labels_df.iloc[idx, 1]\n",
        "        label = self.classmapper[label]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n_Tx--KymjaQ",
      "metadata": {
        "id": "n_Tx--KymjaQ"
      },
      "source": [
        "MEDIC data-FILTER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56-5StIW0SqJ",
      "metadata": {
        "id": "56-5StIW0SqJ"
      },
      "outputs": [],
      "source": [
        "def filter_for_existing_paths(df):\n",
        "    print(f'Number of rows before filtering for existing paths: {len(df)}')\n",
        "    all_paths = df[['image_path']].values.tolist()\n",
        "    non_existent_paths = []\n",
        "    for path in all_paths:\n",
        "        path = path[0]\n",
        "        full_path = os.path.join(os.getcwd(), path)\n",
        "        if not os.path.exists(full_path):\n",
        "            non_existent_paths.append(path)\n",
        "    df = df[~df['image_path'].isin(non_existent_paths)]\n",
        "    print(f'Number of rows after filtering for existing paths: {len(df)}')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OKbZ3_33mzOw",
      "metadata": {
        "id": "OKbZ3_33mzOw"
      },
      "source": [
        "MEDIC data-FILTER & READING "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1997a624",
      "metadata": {
        "id": "1997a624"
      },
      "outputs": [],
      "source": [
        "def generate_medic_filtered_df(type_dataset):\n",
        "    df = pd.read_csv(os.path.join(os.getcwd(),f'data_disaster_types/consolidated_disaster_types_{type_dataset}_final.tsv'), sep='\\t', header=0)\n",
        "    print(f'Number of rows from original MEDIC {type_dataset} file: {len(df)}')\n",
        "    condition_1 = df['class_label'] == 'flood'\n",
        "    condition_2 = df['class_label'] == 'fire'\n",
        "    condition_3 = df['class_label'] == 'not_disaster'\n",
        "    filtered_df = df[condition_1 | condition_2 |  condition_3]\n",
        "    filtered_df = filtered_df[['image_path', 'class_label']]\n",
        "    filtered_df['image_path'] = 'data_disaster_types/' + df['image_path'].astype(str) # in preparation for combining\n",
        "    final_df = filter_for_existing_paths(filtered_df)\n",
        "    return final_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pGH9wsOym8Un",
      "metadata": {
        "id": "pGH9wsOym8Un"
      },
      "source": [
        "DATA FRAME COMBINED: MEDIC & AIDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qI7_gZtm99Fm",
      "metadata": {
        "id": "qI7_gZtm99Fm"
      },
      "outputs": [],
      "source": [
        "AIDER_range = {\n",
        "    'train': {\n",
        "        'flood': [101, 526], #0\n",
        "        'fire': [101,521], #0\n",
        "        'normal': [1001,4390]\n",
        "    },\n",
        "    'val': {\n",
        "        'flood': [1, 100], #0001 to 0100\n",
        "        'fire': [1,100], #0001 to 0100\n",
        "        'normal': [1,1000] # 0001\n",
        "    }\n",
        "}\n",
        "\n",
        "def combine_df(df, type_dataset):\n",
        "    ranges = AIDER_range[type_dataset]\n",
        "    all_info = []\n",
        "    for disaster_type in ranges: # range is flood, fire, normal\n",
        "        if disaster_type == 'normal':\n",
        "            normalized_disaster_type = 'not_disaster'\n",
        "        else:\n",
        "            normalized_disaster_type = disaster_type\n",
        "        lower = ranges[disaster_type][0]\n",
        "        upper = ranges[disaster_type][1]\n",
        "        for index in range(lower, upper+1): # for each disaster type\n",
        "            str_index = str(index)\n",
        "            num_zeros_before = 4 - len(str_index)\n",
        "            str_num = '0' * num_zeros_before + str(index)\n",
        "            image_path = f'AIDER_filtered/{type_dataset}/{disaster_type}/{disaster_type}_image{str_num}.jpg'\n",
        "            all_info.append([image_path, normalized_disaster_type]) \n",
        "    new_df = pd.DataFrame(all_info, columns=['image_path', 'class_label'])\n",
        "    combined_df = pd.concat([df, new_df])\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cXSy_w-nUfR",
      "metadata": {
        "id": "0cXSy_w-nUfR"
      },
      "source": [
        "DATA SETS COMBINED: MEDIC & AIDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b1532c5",
      "metadata": {
        "id": "6b1532c5"
      },
      "outputs": [],
      "source": [
        "def init_custom_dataset(train_df, val_df):\n",
        "    classes = ['fire','flood', 'not_disaster']\n",
        "    idx_to_class = {i:j for i,j in enumerate(classes)}\n",
        "    class_to_idx = {v:k for k,v in idx_to_class.items()}\n",
        "\n",
        "    data_transforms = get_data_transforms()\n",
        "\n",
        "    train_dataset = CustomDataset(labels_df=train_df,\n",
        "                              data_dir=os.getcwd(),\n",
        "                              class_mapper=class_to_idx,\n",
        "                              transform = data_transforms['train'])\n",
        "\n",
        "    val_dataset = CustomDataset(labels_df=val_df,\n",
        "                              data_dir=os.getcwd(),\n",
        "                              class_mapper=class_to_idx,\n",
        "                              transform = data_transforms['val'])\n",
        "    return train_dataset, val_dataset, idx_to_class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CdSx9XeNngU5",
      "metadata": {
        "id": "CdSx9XeNngU5"
      },
      "source": [
        "DATALOADERS FOR ALL DATA SETS(TRAIN & VALIDATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "251c92dc",
      "metadata": {
        "id": "251c92dc"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training and validation sets\n",
        "#num_workers:tells dataloader instane how many sub-processes to use for data loading. If zero, GPU has to weight for CPU\n",
        "#load data.greater num_workers more efficiently the CPU load data and less the GPU has to wait. \n",
        "#Google Colab: suggested num_workers=2\n",
        "def init_dataloaders(train_dataset, val_dataset, batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                              shuffle=False, num_workers=2)\n",
        "\n",
        "    # Set up dict for dataloaders\n",
        "    dataloaders = {'train':train_loader,'val':val_loader}\n",
        "    # Store size of training and validation sets\n",
        "    dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
        "    return dataloaders, dataset_sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gxtlHnTXn-Y3",
      "metadata": {
        "id": "gxtlHnTXn-Y3"
      },
      "source": [
        "GENERATING AIDER & MEDIC DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eP3kVhd_zld",
      "metadata": {
        "id": "4eP3kVhd_zld"
      },
      "outputs": [],
      "source": [
        "def generate_aider_datasets():\n",
        "    data_transforms = get_data_transforms()\n",
        "    aider_train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
        "    aider_val_dataset= datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/val'), data_transforms['val'])\n",
        "    class_names = aider_train_dataset.classes\n",
        "    return aider_train_dataset, aider_val_dataset, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ux2XM-XxAGFV",
      "metadata": {
        "id": "Ux2XM-XxAGFV"
      },
      "outputs": [],
      "source": [
        "def generate_medic_datasets():\n",
        "    medic_train_df = generate_medic_filtered_df('train')\n",
        "    medic_val_df = generate_medic_filtered_df('test')\n",
        "    medic_train_dataset, medic_val_dataset, idx_to_class = init_custom_dataset(medic_train_df, medic_val_df)\n",
        "    return medic_train_dataset, medic_val_dataset, medic_train_df, medic_val_df, idx_to_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_x-SI1FRBEfW",
      "metadata": {
        "id": "_x-SI1FRBEfW"
      },
      "outputs": [],
      "source": [
        "def generate_combined_datasets(medic_train_df, medic_val_df):\n",
        "    combined_train_df = combine_df(medic_train_df, 'train')\n",
        "    combined_val_df = combine_df(medic_val_df, 'val')\n",
        "    combined_train_dataset, combine_val_dataset, _ = init_custom_dataset(combined_train_df, combined_val_df)\n",
        "    return combined_train_dataset, combine_val_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c828e0be",
      "metadata": {},
      "source": [
        "IMAGES FOR COMBINED MEDIC & AIDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a412c2a7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_images(loader, mapping, batch_size, tensor):\n",
        "    images, labels = iter(loader).next()\n",
        "    images = images.numpy()\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "    for idx in range(batch_size):\n",
        "        ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        label = labels[idx]\n",
        "        if tensor:\n",
        "            label = label.item()\n",
        "        ax.set_title(\"{}\".format(mapping[label]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37dbd47",
      "metadata": {
        "id": "d37dbd47"
      },
      "source": [
        "\n",
        "Model Architecture: ResNet50 with transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db2bbca",
      "metadata": {},
      "outputs": [],
      "source": [
        "def instantiate_train_visualize(net, dataloaders, dataset_sizes, mapping, tensor):\n",
        "  # Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
        "  for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "  # Get the number of inputs to final Linear layer\n",
        "  num_ftrs = net.fc.in_features\n",
        "  #print(num_ftrs)\n",
        "  # Replace final Linear layer with a new Linear with the same number of inputs but just 3 outputs,\n",
        "  # since we have 3 classes - fire, flood and normal or diaster\n",
        "  net.fc = nn.Linear(num_ftrs, 3)\n",
        "\n",
        "  # Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Define optimizer\n",
        "  optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  # Learning rate scheduler - decay LR by a factor of 0.1 every 7 epochs\n",
        "  lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "  # Set device\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # Train the model\n",
        "  net = train_model(net, criterion, optimizer, dataloaders, lr_scheduler, device, dataset_sizes, num_epochs=2)\n",
        "\n",
        "  # Visualize results \n",
        "  visualize_results(net, dataloaders['val'], device, mapping, tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f23567",
      "metadata": {
        "id": "b3f23567"
      },
      "source": [
        "## Train model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62ad0ac",
      "metadata": {
        "id": "e62ad0ac"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, scheduler, device, dataset_sizes, num_epochs=25):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the weight gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass to get outputs and calculate loss\n",
        "                # Track gradient only for training data\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backpropagation to get the gradients with respect to each weight\n",
        "                    # Only if in train\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        # Update the weights\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Convert loss into a scalar and add it to running_loss\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # Track number of correct predictions\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Step along learning rate scheduler when in train\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calculate and display average loss and accuracy for the epoch\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # If model performs better on val set, save weights as the best model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:3f}'.format(best_acc))\n",
        "\n",
        "    # Load the weights from best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9086ea10",
      "metadata": {},
      "source": [
        "Calculating predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "881993d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(model,test_loader,device):\n",
        "    model = model.to(device)\n",
        "    # Turn autograd off\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Set up lists to store true and predicted values\n",
        "        y_true = []\n",
        "        test_preds = []\n",
        "\n",
        "        # Calculate the predictions on the test set and add to list\n",
        "        for data in test_loader:\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Feed inputs through model to get raw scores\n",
        "            logits = model.forward(inputs)\n",
        "            # Convert raw scores to probabilities (not necessary since we just care about discrete probs in this case)\n",
        "            probs = F.softmax(logits,dim=1)\n",
        "            # Get discrete predictions using argmax\n",
        "            preds = np.argmax(probs.numpy(),axis=1)\n",
        "            # Add predictions and actuals to lists\n",
        "            test_preds.extend(preds)\n",
        "            y_true.extend(labels)\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        test_preds = np.array(test_preds)\n",
        "        y_true = np.array(y_true)\n",
        "        test_acc = np.sum(test_preds == y_true)/y_true.shape[0]\n",
        "        \n",
        "        # Recall for each class\n",
        "        recall_vals = []\n",
        "        for i in range(10):\n",
        "            class_idx = np.argwhere(y_true==i)\n",
        "            total = len(class_idx)\n",
        "            correct = np.sum(test_preds[class_idx]==i)\n",
        "            recall = correct / total\n",
        "            recall_vals.append(recall)\n",
        "    \n",
        "    return test_acc,recall_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1t7au9nBojpH",
      "metadata": {
        "id": "1t7au9nBojpH"
      },
      "source": [
        "PREDICTIONS DISPLAY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1a9a7b",
      "metadata": {
        "id": "6c1a9a7b"
      },
      "outputs": [],
      "source": [
        "# Display a batch of predictions\n",
        "def visualize_results(model,val_loader, device, mapping, tensor):\n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Get a batch of validation images\n",
        "        images, labels = iter(val_loader).next()\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        # Get predictions\n",
        "        _,preds = torch.max(model(images), 1)\n",
        "        preds = np.squeeze(preds.cpu().numpy())\n",
        "        images = images.cpu().numpy()\n",
        "\n",
        "    # Plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    for idx in range(len(preds)):\n",
        "        ax = fig.add_subplot(2, len(preds)//2, idx+1, xticks=[], yticks=[])\n",
        "        image = images[idx]\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        image = std * image + mean\n",
        "        image = np.clip(image, 0, 1)\n",
        "        ax.imshow(image)\n",
        "        prediction = preds[idx]\n",
        "        actual =  labels[idx]\n",
        "        if tensor:\n",
        "            prediction = prediction.item()\n",
        "            actual = actual.item()\n",
        "        ax.set_title(\"{} ({})\".format(mapping[prediction], mapping[actual]),\n",
        "                    color=(\"green\" if prediction==actual else \"red\"))\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6-mE049pEwa",
      "metadata": {
        "id": "f6-mE049pEwa"
      },
      "source": [
        "MAIN FUNCTION:COMBINES MEDIC & AIDER "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0rX99NAQ7h7m",
      "metadata": {
        "id": "0rX99NAQ7h7m"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    aider_train_dataset, aider_val_dataset, class_names = generate_aider_datasets()\n",
        "    medic_train_dataset, medic_val_dataset, medic_train_df, medic_val_df, idx_to_class = generate_medic_datasets()\n",
        "    combined_train_dataset, combined_val_dataset = generate_combined_datasets(medic_train_df, medic_val_df)\n",
        "\n",
        "    data_sources = {\n",
        "      'AIDER': {\n",
        "          'train': aider_train_dataset,\n",
        "          'val': aider_val_dataset\n",
        "      },\n",
        "      'MEDIC': {\n",
        "          'train': medic_train_dataset,\n",
        "          'val': medic_val_dataset\n",
        "      },\n",
        "      'combined': {\n",
        "          'train': combined_train_dataset,\n",
        "          'val': combined_val_dataset\n",
        "      }\n",
        "    }\n",
        "\n",
        "    models = [torchvision.models.resnet50(pretrained=True)]\n",
        "    batch_size = 4\n",
        "\n",
        "    for data_source in data_sources:\n",
        "        train_dataset = data_sources[data_source]['train']\n",
        "        val_dataset =  data_sources[data_source]['val']\n",
        "        dataloaders, dataset_sizes = init_dataloaders(train_dataset, val_dataset, batch_size)\n",
        "        print(f'{data_source}: {dataset_sizes}')\n",
        "        if data_source == 'AIDER':\n",
        "            mapping = class_names\n",
        "            tensor = False\n",
        "        else:\n",
        "            mapping = idx_to_class \n",
        "            tensor = True\n",
        "        # \"Issue\" with displaying: Only displays last images, may need to save dl and cn and run in separate cells \n",
        "        display_images(dataloaders['train'], mapping, batch_size, tensor)\n",
        "        display_images(dataloaders['val'], mapping, batch_size, tensor)\n",
        "        for model in models:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            instantiate_train_visualize(model, dataloaders, dataset_sizes, mapping, tensor)\n",
        "            acc,recall_vals = test_model(model,dataloaders['val'],device)\n",
        "            print('Test set accuracy is {:3f}'.format(acc))\n",
        "            for i in range(3):\n",
        "                print('For class {}, recall is {}'. format(mapping[i], recall_vals[i]))\n",
        "\n",
        "            if data_source ==\"combined\": \n",
        "              savemodel(model)\n",
        "            print('-' * 20)\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266bc32f",
      "metadata": {
        "id": "266bc32f"
      },
      "source": [
        "## Evaluate Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3fe68cd",
      "metadata": {
        "id": "e3fe68cd"
      },
      "source": [
        "## Report on Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b497e8d",
      "metadata": {
        "id": "5b497e8d"
      },
      "outputs": [],
      "source": [
        "# Performance, recall, F1-score\n",
        "from sklearn.metrics import classification_report\n",
        "#print(classification_report(medic, aider))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "848cf03e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import plot_confusion_matrix\n",
        "plot_confusion_matrix, aider, medic, cmap = plt.cm.Blues, normalize=None)\n",
        "plt.show() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sSiOYLrpi524",
      "metadata": {
        "id": "sSiOYLrpi524"
      },
      "source": [
        "Save Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73aec27c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Save the entire model\n",
        "def savemodel(net):\n",
        "  model_dir = '/content/AIPI-540-CV-Team-2-Project/models/fullmodel.pt'\n",
        "  #os.makedirs(os.path.dirname(model_dir), exist_ok=True)\n",
        "  filename = 'fullmodel.pt'\n",
        "\n",
        "# Save the entire model\n",
        "  torch.save(net, model_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d2afb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "def loadmodel(model2, testloader,device,path_to_data,mapping,tensor):\n",
        "  model2 = torch.load(path_to_data)\n",
        "\n",
        "# Test loaded model\n",
        "  acc = visualize_results(model2,testloader,device,mapping,tensor)\n",
        "  print('Test set accuracy is {:.3f}'.format(acc))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Explore.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "f6b7b02a527123a58752ed98c09a696410bd0a26e4f23108e96487035cbed0e2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
