{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc08a90",
   "metadata": {},
   "source": [
    "# Computer Vision Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cc37ea",
   "metadata": {},
   "source": [
    "#### Approach we will take for the Aerial vs Ground Natural Disaster computer vision project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bc598",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8880aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this from GitHub Repository / Random Class Notebooks / etc.\n",
    "\n",
    "# At minimum, we need PyTorch\n",
    "\n",
    "# Run this cell only if working in Colab\n",
    "# Connects to any needed files from GitHub and Google Drive\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# Remove Colab default sample_data\n",
    "!rm -r ./sample_data\n",
    "\n",
    "# # Clone GitHub files to colab workspace\n",
    "git_user = \"sfhorng\" # Enter user or organization name\n",
    "git_email = \"sh390@duke.edu\" # Enter your email\n",
    "repo_name = \"AIPI-540-CV-Team-2-Project\" # Enter repo name\n",
    "# # Use the below if repo is private, or is public and you want to push to it\n",
    "# # Otherwise comment next two lines out\n",
    "git_token = getpass.getpass(\"enter git token\") # Enter your github token \n",
    "git_path = f\"https://{git_token}@github.com/{git_user}/{repo_name}.git\"\n",
    "!git clone \"{git_path}\"\n",
    "\n",
    "# Install dependencies from requirements.txt file\n",
    "notebook_dir = 'notebooks'\n",
    "!pip install -r \"{os.path.join(repo_name,'requirements.txt')}\"\n",
    "\n",
    "# Change working directory to location of notebook\n",
    "\n",
    "path_to_data = os.path.join(repo_name,'data/raw')\n",
    "%cd \"{path_to_data}\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da624728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pydicom\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e33435",
   "metadata": {},
   "source": [
    "## Load data to our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d08df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the following notebooks:\n",
    "\n",
    "\n",
    "# Image classification (writing neural network from scratch)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Stephanie\n",
    "# path = \"/content/drive/My Drive/AIPI-540-Team-2-CV-Project-Datasets/Filtered/AIDER_filtered.zip\"\n",
    "#Amani\n",
    "path=\"/content/drive/My Drive/AIDER_filtered.zip\"\n",
    "zip_ref = zipfile.ZipFile(path, 'r')\n",
    "zip_ref.extractall(os.path.join(os.getcwd()))\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855c406",
   "metadata": {},
   "source": [
    "## Data Preparation: \n",
    "### Set up class to add labels, manipulate the images to size correctly, data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecad6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIDER - use approach based on image classification notebook (writing neural network from scratch)\n",
    "# MEDIC - need to use DICOM\n",
    "\n",
    "# labels = Aerial and Ground. Need to standardize \"Fire\", \"Flood\", \"No Disaster\" across both datasets so labels are the same\n",
    "\n",
    "# Reference code to size images correctly. May not be in correct section, \n",
    "# but we might need to compare image sizes across our two data sets and standardize?\n",
    "\n",
    "# Data augmentation to create new images from AIDER data set to get better class balance between aerial and ground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c26571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up transformations for training and validation (test) data\n",
    "# For training data we will do randomized cropping to get to 224 * 224, randomized horizontal flipping, and normalization\n",
    "# For test set we will do only center cropping to get to 224 * 224 and normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
    "#val_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets for traingin and validation sets--AIDER\n",
    "train_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(os.path.join(os.getcwd(), 'AIDER_filtered/train'), data_transforms['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c92dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training and validation sets\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=4)\n",
    "\n",
    "# Set up dict for dataloaders\n",
    "dataloaders = {'train':train_loader,'val':val_loader}\n",
    "# Store size of training and validation sets\n",
    "dataset_sizes = {'train':len(train_dataset),'val':len(val_dataset)}\n",
    "# Get class names associated with labels\n",
    "class_names = train_dataset.classes\n",
    "#print(class_names)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b41a4d",
   "metadata": {},
   "source": [
    "Images-AIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = iter(train_loader).next()\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc8476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data inspection\n",
    "print(f\"Feature batch shape: {train_dataset.size()}\")\n",
    "print(f\"Feature batch shape: {val_dataset.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57da028",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0dea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEDIC:\n",
    "## Choose which pictures to keep from MEDIC (there are a lot more than AIDER)\n",
    "## Drop pictures we don't want from MEDIC\n",
    "## Drop Mild Category\n",
    "## Drop Not Informative label if they exist?\n",
    "## Only Keep fires, floods, and not disasters\n",
    "\n",
    "#AIDER:\n",
    "## Only keep fires, floods and not disasters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45182f18",
   "metadata": {},
   "source": [
    "## Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a0b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge MEDIC and AIDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0cefa",
   "metadata": {},
   "source": [
    "## Train_test_split into our training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42be63a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dbd47",
   "metadata": {},
   "source": [
    "# Amani:\n",
    "## Set up Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f35855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to decide on best architecture to use based on our goals. Performance/computational resources tradeoffs\n",
    "\n",
    "# Are there pre-trained models that are less computationally heavy to start off with? Research this & include in PowerPoint\n",
    "# Transfer Learning ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f23567",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3b234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference existing code to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266bc32f",
   "metadata": {},
   "source": [
    "## Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2a7e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation / Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe68cd",
   "metadata": {},
   "source": [
    "## Report on Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b497e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance, recall, F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98f584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground vs Aerial analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6b7b02a527123a58752ed98c09a696410bd0a26e4f23108e96487035cbed0e2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DeepLearningSummer2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
